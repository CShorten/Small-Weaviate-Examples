{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers sentence-transformers > /dev/null"
      ],
      "metadata": {
        "id": "XK-yveDgEb_V"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "cYJQ6LYoD-Db",
        "outputId": "99bd33c5-691f-46f4-88f0-204f07ce5241"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name bert-base-uncased. Creating a new one with MEAN pooling.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertConfig, BertForMaskedLM\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "mlm_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "mlm_model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "mlm_head = mlm_model.cls\n",
        "\n",
        "mlm_model.save_pretrained(\"bert-base-uncased\")\n",
        "mlm_tokenizer.save_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "encoder = SentenceTransformer(\"bert-base-uncased\")\n",
        "\n",
        "import torch\n",
        "#text = \"Dogs generally enjoy peanut butter\" \n",
        "text = \"Paris is the capital of France\"\n",
        "representation = torch.tensor(encoder.encode([text]))\n",
        "sparse_vector = mlm_head(representation)\n",
        "\n",
        "mlm_tokenizer.decode([torch.argmax(sparse_vector)])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = \"The capital of the United States is [MASK].\""
      ],
      "metadata": {
        "id": "drKtq8gATRk7"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "inputs = tokenizer(test, return_tensors=\"pt\")\n",
        "\n",
        "logits = model(**inputs).logits\n",
        "\n",
        "# retrieve index of [MASK]\n",
        "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
        "\n",
        "#predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
        "predicted_token_id = torch.argmax(logits[0, mask_token_index])\n",
        "tokenizer.decode([predicted_token_id])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "q3wUAS_nM1c-",
        "outputId": "c4898566-4cf7-4ff0-a4ba-4a1290cd6837"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'atlanta'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pooled_input = encoder.encode([test])\n",
        "projector = model.cls\n",
        "projected_logits = projector(torch.tensor(pooled_input))\n",
        "\n",
        "logit_density = {}\n",
        "\n",
        "for logit_idx, density_val in enumerate(projected_logits[0].tolist()):\n",
        "  logit_density[logit_idx] = density_val\n",
        "\n",
        "sorted_scores = sorted(logit_density.items(), key = lambda item: item[1], reverse=True)\n",
        "\n",
        "for idx in range(50):\n",
        "  print(tokenizer.decode([sorted_scores[idx][0]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dq_87xOeR4J3",
        "outputId": "3006bb34-c6c6-4855-e0bb-17c547861f24"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".\n",
            "of\n",
            "the\n",
            "capital\n",
            "city\n",
            ",\n",
            "in\n",
            "is\n",
            "s\n",
            "state\n",
            "county\n",
            "and\n",
            "country\n",
            "'\n",
            "\"\n",
            "-\n",
            "united\n",
            "its\n",
            "washington\n",
            "southern\n",
            "a\n",
            "it\n",
            "american\n",
            ";\n",
            "california\n",
            "to\n",
            "center\n",
            "where\n",
            "largest\n",
            "federal\n",
            "for\n",
            "nation\n",
            "states\n",
            "metropolitan\n",
            "president\n",
            "by\n",
            "new\n",
            "northern\n",
            "canada\n",
            "union\n",
            "mexico\n",
            "(\n",
            ":\n",
            "government\n",
            "oklahoma\n",
            "national\n",
            "most\n",
            "liberty\n",
            "second\n",
            "usa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xtAHr-eVSjSj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}