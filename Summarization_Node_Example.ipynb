{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NKNJTn9iTio_"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer(\"sentence-transformers/allenai-specter\")"
      ],
      "metadata": {
        "id": "guDR9bhMT4wH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abstract_plus_intro = [\n",
        "    \"Dual encoders are now the dominant architecture for dense retrieval. Yet, we have little understanding of how they represent text, and why this leads to good performance. In this work, we shed light on this question via distributions over the vocabulary. We propose to interpret the vector representations produced by dual encoders by projecting them into the model’s vocabulary space. We show that the resulting distributions over vocabulary tokens are intuitive and contain rich semantic information. We find that this view can explain some of the failure cases of dense retrievers. For example, the inability of models to handle tail entities can be explained via a tendency of the token distributions to forget some of the tokens of those entities. We leverage this insight and propose a simple way to enrich query and passage representations with lexical information at inference time, and show that this significantly improves performance compared to the original model in out-of-domain settings.\",\n",
        "    \"Dense retrieval models based on neural text representations have proven very effective (Karpukhin et al., 2020; Qu et al., 2021; Ram et al., 2022; Izacard et al., 2022a,b), improving upon strong traditional sparse models like BM25 (Robertson and Zaragoza, 2009). However, when applied off-the-shelf (i.e., in out-of-domain settings) they often experience a severe drop in performance (Thakur et al., 2021; Sciavolino et al., 2021; Reddy et al., 2021). Moreover, the reasons for such failures are poorly understood, as the mechanism underlying dense representations remains under-investigated.\",\n",
        "    \"In this work, we present a new approach for interpreting and reasoning about dense retrievers, through distributions induced by their query (throughout the paper, we use query and question interchangeably) and passage representations when projected to the vocabulary space, namely distributions over their vocabulary space (Figure 1). Such distributions enable a better understanding of the representational nature of dense models and their failures, which paves the way to simple solutions that improve their performance.\",\n",
        "    \"Figure 1 - An example of our framework. We run the question “Where was Michael Jack born?” through the question encoder of DPR (Karpukhin et al., 2020), and project the question representation eq to the vocabulary space using BERT’s masked language modeling head (Devlin et al., 2019). The result is a distribution over the vocabulary, Q. We apply the same procedure for passages as well. These projections enable reasoning about and improving retrieval representations.\",\n",
        "    \"We begin by showing that dense retrieval representations can be projected to the vocabulary space, by feeding them through the masked language modeling (MLM) head of the pretrained model they were initialized from without any further training. This operation results in distributions over the vocabulary, which we refer to as query vocabulary projections and passage vocabulary projections.\",\n",
        "    \"Surprisingly, we find these projections to be highly interpretable to humans (Figure 2; Table 1). First, we analyze query projections and show that they contain both tokens that appear in the queries themselves, as well as additional tokens that are likely to appear in relevant passages. In other words, the model implicitly implements query expansion (Rocchio, 1971). For example, in Figure 2 the query is 'How many judges currently serve on the Supreme court?', and the words in the query projection Q include 'justices' a synonym of 'judges'. An even more intriguing type of expansion is one where the added words actually contain information about the answer. For example, the word 'nine' is also ranked high in Q even though it does not appear in the question itself, and nine is actually the correct answer.\",\n",
        "    \"Figure 2 - Figure 2: A success case from Natural Questions (top) and a failure case from EntityQuestions (bottom) of DPR (Karpukhin et al., 2020), explained via projecting question and (its relevant) passage representations to the vocabulary space. Tokens in the top-20 of both question and passage vocabulary projections are marked in bold.\",\n",
        "    \"We then continue to analyze passage projections, and show that they are likely to contain words that appear in queries about this passage. Thus, the passage projections can be viewed as anticipating the questions one would ask about the passage.\",\n",
        "    \"The above findings are especially surprising due to the fact that these retrieval models are fine-tuned in a contrastive fashion, and thus do not perform any prediction over the vocabulary or make any use of their language modeling head during fine-tuning. In addition, these representations are the result of running a deep transformer network that can implement highly complex functions. Nonetheless, model outputs remain 'loyal' to the original lexical space learned during pretraining.\",\n",
        "    \"We further show that our approach is able to shed light on the reasons for which dense retrievers struggle with simple entity-centric questions (Sciavolino et al., 2021). Through the lens of vocabulary pro- jections, we identify an interesting phenomenon: dense retrievers tend to “ignore” some of the tokens appearing in a given passage. This is reflected in the ranking assigned to such tokens in the passage projection. For example, the word 'michael' in the bottom example of Figure 2 is ranked relatively low (even though it appears in the passage title), thereby hindering the model from retrieving this passage. We refer to this syndrome as token amnesia.\",\n",
        "    \"We leverage this insight and suggest a simple inference-time fix that enriches dense representations with lexical information, addressing token amnesia. We show that lexical enrichment signifi- cantly improves performance compared to vanilla models across multiple datasets. For example, we are able to boost the top-20 retrieval accuracy of DPR (Karpukhin et al., 2020) on the challenging EntityQuestions dataset (Sciavolino et al., 2021) by more than 15 points (from 49.7% to 65.4%).\",\n",
        "    \"Taken together, our analyses and results demonstrate the great potential of vocabulary projections as a framework for more principled research and development of dense retrieval models.\"\n",
        "]"
      ],
      "metadata": {
        "id": "0kww9zPbT-O0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "org_embeds = []\n",
        "for segment in abstract_plus_intro:\n",
        "  org_embeds.append(model.encode(segment))\n",
        "\n",
        "import numpy as np\n",
        "org_embeds = np.array(org_embeds)"
      ],
      "metadata": {
        "id": "6h0XjL69U-Zg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "TSNE_params = TSNE(n_components=2, learning_rate='auto',\n",
        "                  init='random', perplexity=3)\n",
        "\n",
        "tsne_embedded_papers = TSNE_params.fit_transform(org_embeds)"
      ],
      "metadata": {
        "id": "Gr5fR943VfXP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.scatter(tsne_embedded_papers[:, 0], tsne_embedded_papers[:, 1]);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "BhDSlglWV8Ft",
        "outputId": "aba9a332-59ae-4ca1-8c24-d85f460ef2dd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "default: For the given index, \"create and refine\" an answer by sequentially going through each Node; make a separate LLM call per Node. Good for more detailed answers.\n"
      ],
      "metadata": {
        "id": "ZEQFyZunXINQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Begins with the prompt:\n",
        "\n",
        "`Please summarize this scientific paper. You will receive the paper one passage at a time to \"create and refine\" an answer by sequentially going through each passage.`\n",
        "\n",
        "Iteratively using the prompt:\n",
        "\n",
        "`Please add the information in this paragraph to the summary you have outputted so far.`"
      ],
      "metadata": {
        "id": "CTzaRjQTYyBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summaries = [\n",
        "    \"This paper discusses the use of dual encoders, a common architecture for dense retrieval in natural language processing, and how they represent text. The authors propose interpreting the vector representations produced by dual encoders by projecting them into the model's vocabulary space and show that the resulting distributions of vocabulary tokens contain rich semantic information. They also find that this view can explain some failure cases of dense retrieval models and propose a method for improving performance by enriching query and passage representations with lexical information at inference time, which is shown to significantly improve performance in out-of-domain settings.\",\n",
        "    \"This paper discusses the effectiveness of dense retrieval models based on neural text representations, which have outperformed traditional sparse models like BM25. However, these models often experience a significant decrease in performance when applied in out-of-domain settings, and the reasons for these failures are not well understood due to a lack of investigation into the mechanisms underlying dense representations.\",\n",
        "    \"In this work, the authors present a new approach for interpreting and understanding dense retrieval models through the use of distributions induced by the query and passage representations when projected into the vocabulary space. These distributions provide insight into the nature of dense models and their failures, and can be used to develop simple solutions to improve performance.\",\n",
        "    \"The authors propose a framework for interpreting and understanding dense retrieval models through the use of distributions induced by projecting query and passage representations into the vocabulary space. These projections enable analysis of the models and can be used to improve retrieval representations. An example of this process is shown in Figure 1, in which a question is run through the question encoder of a dense retrieval model and projected into the vocabulary space using a masked language modeling head, resulting in a distribution over the vocabulary. This process is also applied to passages.\",\n",
        "    \"\",\n",
        "]"
      ],
      "metadata": {
        "id": "Z0LU2mCKWAcm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Begins with the prompt:\n",
        "\n",
        "``\n",
        "\n",
        "Iteratively using the prompt:\n",
        "\n",
        "`Please update the summary so far: {summary} with this next passage from the paper: {passage}`"
      ],
      "metadata": {
        "id": "8tiV_xG4ZrkE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JWsLfIQLZuY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "compact: For the given index, \"compact\" the prompt during each LLM call by stuffing as many Node text chunks that can fit within the maximum prompt size. If there are too many chunks to stuff in one prompt, \"create and refine\" an answer by going through multiple prompts."
      ],
      "metadata": {
        "id": "gIjDzODYXI-N"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UO6M9UErXJ_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "tree_summarize: Given a set of Nodes and the query, recursively construct a tree and return the root node as the response. Good for summarization purposes."
      ],
      "metadata": {
        "id": "M2TaXlASXLFF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZSDmnoYTXLTW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}